{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m  Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ConnectTimeoutError(<pip._vendor.urllib3.connection.VerifiedHTTPSConnection object at 0x7fc650bab6d0>, 'Connection to pypi.tuna.tsinghua.edu.cn timed out. (connect timeout=15)')': /simple/cornac/\u001b[0m\n",
      "^C\n",
      "\u001b[31mOperation cancelled by user\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#!pip install --quiet cornac==1.6.0 adjustText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System version: 3.6.7 (default, Oct 25 2018, 09:16:13) \n",
      "[GCC 5.4.0 20160609]\n",
      "Cornac version: 1.6.0\n",
      "Tensorflow version: 1.11.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "import nltk\n",
    "\n",
    "import string\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from adjustText import adjust_text\n",
    "%matplotlib inline\n",
    "\n",
    "import cornac\n",
    "from cornac.utils import cache\n",
    "from cornac.datasets import movielens\n",
    "from cornac.eval_methods import RatioSplit\n",
    "from cornac.models import MF, BPR, WMF\n",
    "from cornac.data import Reader, SentimentModality\n",
    "\n",
    "#%tensorflow_version 1.x\n",
    "import tensorflow as tf\n",
    "\n",
    "print(f\"System version: {sys.version}\")\n",
    "print(f\"Cornac version: {cornac.__version__}\")\n",
    "print(f\"Tensorflow version: {tf.__version__}\")\n",
    "\n",
    "SEED = 42\n",
    "VERBOSE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in review file\n",
    "import pandas as pd\n",
    "import gzip\n",
    "import json\n",
    "\n",
    "def parse(path):\n",
    "  g = gzip.open(path, 'rb')\n",
    "  for l in g:\n",
    "    yield json.loads(l)\n",
    "\n",
    "def getDF(path):\n",
    "  i = 0\n",
    "  df = {}\n",
    "  for d in parse(path):\n",
    "    df[i] = d\n",
    "    i += 1\n",
    "  return pd.DataFrame.from_dict(df, orient='index')\n",
    "\n",
    "df_review = getDF('Clothing_Shoes_and_Jewelry_5.json.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_review.to_pickle('df_review.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_review = pd.read_pickle('df_review.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11285464, 12)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_review.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>vote</th>\n",
       "      <th>verified</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>itemid</th>\n",
       "      <th>style</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>image</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.0</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>05 4, 2014</td>\n",
       "      <td>A2IC3NZN488KWK</td>\n",
       "      <td>0871167042</td>\n",
       "      <td>{'Format:': ' Paperback'}</td>\n",
       "      <td>Ruby Tulip</td>\n",
       "      <td>This book has beautiful photos, good and under...</td>\n",
       "      <td>Unique designs</td>\n",
       "      <td>1399161600</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>03 31, 2014</td>\n",
       "      <td>A30FG02C424EJ5</td>\n",
       "      <td>0871167042</td>\n",
       "      <td>{'Format:': ' Paperback'}</td>\n",
       "      <td>NWCancerBaby</td>\n",
       "      <td>Loved their approach in this book and that it ...</td>\n",
       "      <td>Great Book</td>\n",
       "      <td>1396224000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>05 30, 2015</td>\n",
       "      <td>A2G9GWQEWWNQUB</td>\n",
       "      <td>0871167042</td>\n",
       "      <td>{'Format:': ' Paperback'}</td>\n",
       "      <td>Pamelarenee</td>\n",
       "      <td>great</td>\n",
       "      <td>Five Stars</td>\n",
       "      <td>1432944000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>02 21, 2015</td>\n",
       "      <td>A3NI5OGW35SLY2</td>\n",
       "      <td>0871167042</td>\n",
       "      <td>{'Format:': ' Paperback'}</td>\n",
       "      <td>Gail</td>\n",
       "      <td>Always love the way Eva thinks, and there are ...</td>\n",
       "      <td>Great Book!</td>\n",
       "      <td>1424476800</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>01 21, 2015</td>\n",
       "      <td>A1OPRA4NE56EV6</td>\n",
       "      <td>0871167042</td>\n",
       "      <td>{'Format:': ' Paperback'}</td>\n",
       "      <td>carol a inman</td>\n",
       "      <td>Nice patterns</td>\n",
       "      <td>Five Stars</td>\n",
       "      <td>1421798400</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>12 18, 2014</td>\n",
       "      <td>A3M6UXIK7XTA7A</td>\n",
       "      <td>0871167042</td>\n",
       "      <td>{'Format:': ' Paperback'}</td>\n",
       "      <td>CV</td>\n",
       "      <td>love the spirit of the book and some of the pr...</td>\n",
       "      <td>Four Stars</td>\n",
       "      <td>1418860800</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>09 3, 2014</td>\n",
       "      <td>A22ZX01TPWQY4G</td>\n",
       "      <td>0871167042</td>\n",
       "      <td>{'Format:': ' Paperback'}</td>\n",
       "      <td>Nonnie</td>\n",
       "      <td>Not a fan. Limited designs. Limited content fo...</td>\n",
       "      <td>Two Stars</td>\n",
       "      <td>1409702400</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3.0</td>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "      <td>07 23, 2014</td>\n",
       "      <td>A1YIEW86G14BHP</td>\n",
       "      <td>0871167042</td>\n",
       "      <td>{'Format:': ' Paperback'}</td>\n",
       "      <td>Aldne Now</td>\n",
       "      <td>If you're an experienced wire jewelry maker, t...</td>\n",
       "      <td>OK Jewelry Book</td>\n",
       "      <td>1406073600</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4.0</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>06 5, 2014</td>\n",
       "      <td>AA7PNT2OPS3RP</td>\n",
       "      <td>0871167042</td>\n",
       "      <td>{'Format:': ' Paperback'}</td>\n",
       "      <td>Catherine Franz</td>\n",
       "      <td>Hi, have been making jewelry now for the last ...</td>\n",
       "      <td>Organic and upcycling jewelry maker</td>\n",
       "      <td>1401926400</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5.0</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>05 30, 2014</td>\n",
       "      <td>A3LOIIIW4G3TL7</td>\n",
       "      <td>0871167042</td>\n",
       "      <td>{'Format:': ' Paperback'}</td>\n",
       "      <td>Picky Virgo</td>\n",
       "      <td>There's nothing prissy or matchy-matchy about ...</td>\n",
       "      <td>Perfect Title!</td>\n",
       "      <td>1401408000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rating vote  verified   reviewTime      reviewerID      itemid  \\\n",
       "0     5.0    2      True   05 4, 2014  A2IC3NZN488KWK  0871167042   \n",
       "1     5.0  NaN      True  03 31, 2014  A30FG02C424EJ5  0871167042   \n",
       "2     5.0  NaN      True  05 30, 2015  A2G9GWQEWWNQUB  0871167042   \n",
       "3     5.0  NaN      True  02 21, 2015  A3NI5OGW35SLY2  0871167042   \n",
       "4     5.0  NaN      True  01 21, 2015  A1OPRA4NE56EV6  0871167042   \n",
       "5     4.0  NaN      True  12 18, 2014  A3M6UXIK7XTA7A  0871167042   \n",
       "6     2.0  NaN      True   09 3, 2014  A22ZX01TPWQY4G  0871167042   \n",
       "7     3.0    5      True  07 23, 2014  A1YIEW86G14BHP  0871167042   \n",
       "8     4.0    3      True   06 5, 2014   AA7PNT2OPS3RP  0871167042   \n",
       "9     5.0    2      True  05 30, 2014  A3LOIIIW4G3TL7  0871167042   \n",
       "\n",
       "                       style     reviewerName  \\\n",
       "0  {'Format:': ' Paperback'}       Ruby Tulip   \n",
       "1  {'Format:': ' Paperback'}     NWCancerBaby   \n",
       "2  {'Format:': ' Paperback'}      Pamelarenee   \n",
       "3  {'Format:': ' Paperback'}             Gail   \n",
       "4  {'Format:': ' Paperback'}    carol a inman   \n",
       "5  {'Format:': ' Paperback'}               CV   \n",
       "6  {'Format:': ' Paperback'}           Nonnie   \n",
       "7  {'Format:': ' Paperback'}        Aldne Now   \n",
       "8  {'Format:': ' Paperback'}  Catherine Franz   \n",
       "9  {'Format:': ' Paperback'}      Picky Virgo   \n",
       "\n",
       "                                          reviewText  \\\n",
       "0  This book has beautiful photos, good and under...   \n",
       "1  Loved their approach in this book and that it ...   \n",
       "2                                              great   \n",
       "3  Always love the way Eva thinks, and there are ...   \n",
       "4                                      Nice patterns   \n",
       "5  love the spirit of the book and some of the pr...   \n",
       "6  Not a fan. Limited designs. Limited content fo...   \n",
       "7  If you're an experienced wire jewelry maker, t...   \n",
       "8  Hi, have been making jewelry now for the last ...   \n",
       "9  There's nothing prissy or matchy-matchy about ...   \n",
       "\n",
       "                               summary  unixReviewTime image  \n",
       "0                       Unique designs      1399161600   NaN  \n",
       "1                           Great Book      1396224000   NaN  \n",
       "2                           Five Stars      1432944000   NaN  \n",
       "3                          Great Book!      1424476800   NaN  \n",
       "4                           Five Stars      1421798400   NaN  \n",
       "5                           Four Stars      1418860800   NaN  \n",
       "6                            Two Stars      1409702400   NaN  \n",
       "7                      OK Jewelry Book      1406073600   NaN  \n",
       "8  Organic and upcycling jewelry maker      1401926400   NaN  \n",
       "9                       Perfect Title!      1401408000   NaN  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_review.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_review = df_review.rename(columns = {'asin':'itemid'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_review = df_review.rename(columns = {'overall':'rating'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11285464, 12)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_review.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_review_all = df_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove na in review text\n",
    "df_review_withimage = df_review_all.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['rating', 'vote', 'verified', 'reviewTime', 'reviewerID', 'itemid',\n",
       "       'style', 'reviewerName', 'reviewText', 'summary', 'unixReviewTime',\n",
       "       'image'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_review_withimage.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_review_withtext = df_review_all.dropna(subset=['reviewText'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11275566, 12)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_review_withtext.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample 1000000 rows for processing\n",
    "df_review_withtext_sample = df_review_withtext.sample(n=1000000, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_review_withtext_sample.to_pickle('df_review_withtext_sample.pickle')\n",
    "df_review_withtext_sample = pd.read_pickle('df_review_withtext_sample.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#convert tweets to lower case\n",
    "df_review_withtext_sample['preprocess_data'] = df_review_withtext_sample['reviewText'].str.lower()\n",
    "\n",
    "\n",
    "#removes extra spaces\n",
    "df_review_withtext_sample['preprocess_data'] = df_review_withtext_sample['preprocess_data'].str.replace(r' +', ' ')\n",
    "\n",
    "#punctuation\n",
    "df_review_withtext_sample['preprocess_data'] = df_review_withtext_sample['preprocess_data'].str.replace('[^\\w\\s]','')\n",
    "\n",
    "#split \n",
    "df_review_withtext_sample['preprocess_data'] = df_review_withtext_sample['preprocess_data'].str.lower().str.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9488852                                           [love, it]\n",
       "7802048    [very, poorly, made, zipper, broke, almost, as...\n",
       "6111777    [its, ok, i, returned, it, the, cross, body, s...\n",
       "4670148    [these, fit, perfectly, i, have, issues, with,...\n",
       "9872846    [bought, this, for, my, boyfriend, for, a, goo...\n",
       "Name: preprocess_data, dtype: object"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_review_withtext_sample['preprocess_data'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import pickle\n",
    "with open('stopwords.pickle', \"rb\") as f:\n",
    "    stop = pickle.load(f)\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop.remove('and')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop.remove('but')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stop words removes\n",
    "df_review_withtext_sample['preprocess_data'] = df_review_withtext_sample['preprocess_data'].apply(lambda x: [item for item in x if item not in stop])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert preprocessed list words to string \n",
    "df_review_withtext_sample['preprocess_str'] = df_review_withtext_sample['preprocess_data'].apply(lambda x: ' '.join(x) if x!=0 else x)\n",
    "#df_review_withtext_sample['preprocess_str'] = df_review_withtext_sample['preprocess_data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9488852                                                  love\n",
       "7802048     poorly made zipper broke almost soon started w...\n",
       "6111777     ok returned cross body strap thin side think w...\n",
       "4670148     fit perfectly issues almost tights every tight...\n",
       "9872846     bought boyfriend good laugh but worn and says ...\n",
       "5885524                                                    ok\n",
       "7981666                                             described\n",
       "2314583     reviews and even company said order 12 size im...\n",
       "1824489                             like glasses like picture\n",
       "9466244     advertised runs twice smaller size flimsy look...\n",
       "10239463    love everything shirt except run small conside...\n",
       "9007060                      fit well and look like made well\n",
       "10978671                                          tall person\n",
       "1837233                                             excellent\n",
       "7258321     fit needs grandkids purchased several colors c...\n",
       "8710504     love shoes but seem run little large ordered 8...\n",
       "1104971     like thicker soles dr martens better sole thin...\n",
       "2335353     read reviews and thought good product ordered ...\n",
       "3322362                             get many compliments love\n",
       "1396275                                                  nice\n",
       "10800270    purchased 4 year old granddaughters upcoming t...\n",
       "6802234     great quality bra adjustable straps and provid...\n",
       "466775                                                  works\n",
       "10658605                             perfect fitlove material\n",
       "4491661     slippers still good condition despite constant...\n",
       "3303596     nice large case purchased larger eyeglasses an...\n",
       "7393112     bought 7 year old birthday loved bright colors...\n",
       "9196951     solid quality weekender small duffle bag lots ...\n",
       "5379683     bought husband known wearing slippers however ...\n",
       "4135080     girlfriend moon shoes wont wear anything else ...\n",
       "                                  ...                        \n",
       "10622759                                             way long\n",
       "10970043                                            fun shirt\n",
       "8646409                       product arrive time anc satisfy\n",
       "1953124                           cant beat price and quality\n",
       "5373497     softest tshirt but color and would assume ligh...\n",
       "2504425     love mittens and happy find amazon rare made c...\n",
       "6533266                 still looks good several months later\n",
       "9563646     perfect size and closure easy use put ears mor...\n",
       "10450738    cheap material kinda feels like parachute mate...\n",
       "8863707     good looks little darker quartz anticipated st...\n",
       "6766172     love color little bigger thought arrived earli...\n",
       "8578991     fits nice looks nice job keeping head warm fas...\n",
       "8726686              cute shirt runs small order 1 size busty\n",
       "2125392     simply said pearls nice quality and wife loved...\n",
       "9854008     67 and plump side and wonderful fit and exactl...\n",
       "10734735    love look and feel shoe color beautiful and ha...\n",
       "8225147     read many reviews leggings and finally decided...\n",
       "6166569                                     grand child loves\n",
       "2434430     great product opaque says would got stone colo...\n",
       "11058587    ordered socks wear work work office and consta...\n",
       "2422131                 solid pants 511 great fit and comfort\n",
       "8836781                                                  sexi\n",
       "8172269                                     order bigger size\n",
       "9961583     received bracelet today love perfect smaller w...\n",
       "592029                                          nice and cool\n",
       "11040177    nice but found little scuffed and used slightl...\n",
       "10993076    wow super awesome product best highly recommen...\n",
       "7402889     super woman babys first birthday party wore bo...\n",
       "3778676     perfect thigh high thursday got like amillion ...\n",
       "5388130     great shoes 6yearold liked immediately thought...\n",
       "Name: preprocess_str, Length: 1000000, dtype: object"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_review_withtext_sample['preprocess_str']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/spacy/util.py:275: UserWarning: [W031] Model 'en_core_web_sm' (2.2.0) requires spaCy v2.2 and is incompatible with the current spaCy version (2.3.2). This may lead to unexpected results or runtime errors. To resolve this, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n",
      "/usr/local/lib/python3.6/dist-packages/spacy/util.py:275: UserWarning: [W031] Model 'en_core_web_sm' (2.2.0) requires spaCy v2.2 and is incompatible with the current spaCy version (2.3.2). This may lead to unexpected results or runtime errors. To resolve this, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "#pip3 install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.0/en_core_web_sm-2.2.0.tar.gz\n",
    "import en_core_web_sm\n",
    "\n",
    "nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#aspect terms extraction\n",
    "def pos_tagging(data):\n",
    "    print(\"pos tagging\")\n",
    "    req_tag = ['NN']\n",
    "    extracted_words = []\n",
    "    i = 0\n",
    "    try:\n",
    "        for x in data['preprocess_str']:\n",
    "            doc = nlp(x)\n",
    "            for token in doc:\n",
    "                i += 1\n",
    "                if token.tag_ in req_tag and token.shape_ != 'x' and token.shape_ != 'xx' and token.shape_ != 'xxx':\n",
    "                    extracted_words.append(token.lemma_)\n",
    "        return extracted_words\n",
    "    except Exception as e:\n",
    "        return extracted_words\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_words = pos_tagging(df_review_withtext_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2vec(data):\n",
    "    terms = listoflist(data)\n",
    "    try:\n",
    "        # removal of words which not present in the word2vec model vocabulary. (wrongly spelled)\n",
    "        filtered_terms = []\n",
    "        for i in range(len(terms)):\n",
    "            corrent_words = [token for token in terms[i] if token in model_wiki.wv.vocab]\n",
    "            if len(corrent_words) > 0 :\n",
    "                filtered_terms.append(corrent_words[0])\n",
    "\n",
    "        #converting words into vector\n",
    "        vector_of_terms = []\n",
    "        for x in range(len(filtered_terms)):\n",
    "            vector_of_terms.append(model_wiki.wv[filtered_terms[x]])\n",
    "        return vector_of_terms,filtered_terms\n",
    "    except Exception as e:\n",
    "        return abort(Response(\n",
    "            json.dumps({'status_code': 400, 'success': False, 'message': 'Something went wrong'}),\n",
    "            mimetype=\"application/json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip3 install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.0/en_core_web_sm-2.2.0.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#download from https://gist.github.com/mkulakowski2/4289437\n",
    "text_file = open(\"negative-words.txt\", \"r\")\n",
    "neg = text_file.read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_file = open(\"positive-words.txt\", \"r\")\n",
    "pos = text_file.read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of globally defined positive and negative words to identify sentiment\n",
    "# sentiment score based on the laxicon neg, pos words\n",
    "\n",
    "def feature_sentiment(sentence, pos, neg):\n",
    "    '''\n",
    "    input: dictionary and sentence\n",
    "    function: appends dictionary with new features if the feature\n",
    "              did not exist previously,then updates sentiment to\n",
    "              each of the new or existing features\n",
    "    output: updated dictionary\n",
    "    '''\n",
    "    sent_dict = dict()\n",
    "    sent_tuple_lst = []\n",
    "    sentence = nlp(sentence)\n",
    "    opinion_words = neg + pos\n",
    "    debug = 0\n",
    "    for token in sentence:\n",
    "        # check if the word is an opinion word, then assign sentiment\n",
    "        if token.text in opinion_words:\n",
    "            sentiment = 1 if token.text in pos else -1\n",
    "            # if target is an adverb modifier (i.e. pretty, highly, etc.)\n",
    "            # but happens to be an opinion word, ignore and pass\n",
    "            if (token.dep_ == \"advmod\"):\n",
    "                continue\n",
    "            elif (token.dep_ == \"amod\"):\n",
    "                #sent_dict[token.head.text] = sentiment\n",
    "                sent_tuple_lst.append((token.head.text,token.text,sentiment))\n",
    "            # for opinion words that are adjectives, adverbs, verbs...\n",
    "            else:\n",
    "                for child in token.children:\n",
    "                    # if there's a adj modifier (i.e. very, pretty, etc.) add more weight to sentiment\n",
    "                    # This could be better updated for modifiers that either positively or negatively emphasize\n",
    "                    if ((child.dep_ == \"amod\") or (child.dep_ == \"advmod\")) and (child.text in opinion_words):\n",
    "                        #sentiment *= 1.5\n",
    "                        sentiment *= 1\n",
    "                    # check for negation words and flip the sign of sentiment\n",
    "                    if child.dep_ == \"neg\":\n",
    "                        sentiment *= -1\n",
    "                for child in token.children:\n",
    "                    # if verb, check if there's a direct object\n",
    "                    if (token.pos_ == \"VERB\") & (child.dep_ == \"dobj\"):                        \n",
    "                        #sent_dict[child.text] = sentiment\n",
    "                        sent_tuple_lst.append((child.text,token.text,sentiment))\n",
    "                        # check for conjugates (a AND b), then add both to dictionary\n",
    "                        subchildren = []\n",
    "                        conj = 0\n",
    "                        for subchild in child.children:\n",
    "                            if subchild.text == \"and\":\n",
    "                                conj=1\n",
    "                            if (conj == 1) and (subchild.text != \"and\"):\n",
    "                                subchildren.append(subchild.text)\n",
    "                                conj = 0\n",
    "                        for subchild in subchildren:\n",
    "                            #sent_dict[subchild] = sentiment\n",
    "                            sent_tuple_lst.append((subchild,token.text,sentiment))\n",
    "                # check for negation\n",
    "                for child in token.head.children:\n",
    "                    noun = \"\"\n",
    "                    if ((child.dep_ == \"amod\") or (child.dep_ == \"advmod\")) and (child.text in opinion_words):\n",
    "                        #sentiment *= 1.5\n",
    "                        sentiment *=1\n",
    "                    # check for negation words and flip the sign of sentiment\n",
    "                    if (child.dep_ == \"neg\"): \n",
    "                        sentiment *= -1\n",
    "                \n",
    "                # check for nouns\n",
    "                for child in token.head.children:\n",
    "                    noun = \"\"\n",
    "                    if (child.pos_ == \"NOUN\") and (child.text not in sent_dict):\n",
    "                        noun = child.text\n",
    "                        # Check for compound nouns\n",
    "                        for subchild in child.children:\n",
    "                            if subchild.dep_ == \"compound\":\n",
    "                                noun = subchild.text + \" \" + noun\n",
    "                        #sent_dict[noun] = sentiment\n",
    "                        sent_tuple_lst.append((noun,token.text,sentiment))\n",
    "                    debug += 1\n",
    "              \n",
    "            \n",
    "            \n",
    "    #sent_tuple = [(k, v) for k, v in sent_dict.items()]        \n",
    "    return sent_tuple_lst\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('product', 'wow', 1), ('product', 'super', 1), ('product', 'awesome', 1), ('product', 'recommended', 1)]\n"
     ]
    }
   ],
   "source": [
    "# example \n",
    "sen = \"wow super awesome product best highly recommended\"\n",
    "print (feature_sentiment(sen, pos, neg))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def apply_extraction(row,nlp):\n",
    "    review_body = row['preprocess_str']\n",
    "    userid = row['reviewerID']\n",
    "    itemid =row['itemid']\n",
    "\n",
    "    doc=nlp(review_body)\n",
    "\n",
    "\n",
    "    ## FIRST RULE OF DEPENDANCY PARSE -\n",
    "    ## M - Sentiment modifier || A - Aspect\n",
    "    ## RULE = M is child of A with a relationshio of amod\n",
    "    rule1_pairs = []\n",
    "    for token in doc:\n",
    "        if token.dep_ == \"amod\":\n",
    "            if token.text in pos:\n",
    "                sentiment = 1\n",
    "            elif token.text in neg:\n",
    "                sentiment = -1\n",
    "            else:\n",
    "                sentiment =0\n",
    "        \n",
    "            rule1_pairs.append((token.head.text, token.text,sentiment))\n",
    "\n",
    "    ## SECOND RULE OF DEPENDANCY PARSE -\n",
    "    ## M - Sentiment modifier || A - Aspect\n",
    "    #Direct Object - A is a child of something with relationship of nsubj, while\n",
    "    # M is a child of the same something with relationship of dobj\n",
    "    #Assumption - A verb will have only one NSUBJ and DOBJ\n",
    "\n",
    "    rule2_pairs = []\n",
    "    for token in doc:\n",
    "        children = token.children\n",
    "        A = \"999999\"\n",
    "        M = \"999999\"\n",
    "        for child in children :\n",
    "            if(child.dep_ == \"nsubj\"):\n",
    "                A = child.text\n",
    "            if(child.dep_ == \"dobj\"):\n",
    "                M = child.text\n",
    "        if(A != \"999999\" and M != \"999999\"):\n",
    "            \n",
    "            if M in pos:\n",
    "                sentiment = 1\n",
    "            elif M in neg:\n",
    "                sentiment = -1\n",
    "            else:\n",
    "                sentiment =0\n",
    "            rule2_pairs.append((A, M,sentiment))\n",
    "\n",
    "    ## THIRD RULE OF DEPENDANCY PARSE -\n",
    "    ## M - Sentiment modifier || A - Aspect\n",
    "    #Adjectival Complement - A is a child of something with relationship of nsubj, while\n",
    "    # M is a child of the same something with relationship of acomp\n",
    "    #Assumption - A verb will have only one NSUBJ and DOBJ\n",
    "\n",
    "    rule3_pairs = []\n",
    "\n",
    "    for token in doc:\n",
    "\n",
    "        children = token.children\n",
    "        A = \"999999\"\n",
    "        M = \"999999\"\n",
    "        for child in children :\n",
    "            if(child.dep_ == \"nsubj\"):\n",
    "                A = child.text\n",
    "\n",
    "            if(child.dep_ == \"acomp\"):\n",
    "                M = child.text\n",
    "\n",
    "        if(A != \"999999\" and M != \"999999\"):\n",
    "            \n",
    "            if M in pos:\n",
    "                sentiment = 1\n",
    "            elif M in neg:\n",
    "                sentiment = -1\n",
    "            else:\n",
    "                sentiment =0\n",
    "            rule3_pairs.append((A, M,sentiment))\n",
    "\n",
    "    ## FOURTH RULE OF DEPENDANCY PARSE -\n",
    "    ## M - Sentiment modifier || A - Aspect\n",
    "\n",
    "    #Adverbial modifier to a passive verb - A is a child of something with relationship of nsubjpass, while\n",
    "    # M is a child of the same something with relationship of advmod\n",
    "\n",
    "    #Assumption - A verb will have only one NSUBJ and DOBJ\n",
    "\n",
    "    rule4_pairs = []\n",
    "    for token in doc:\n",
    "\n",
    "\n",
    "        children = token.children\n",
    "        A = \"999999\"\n",
    "        M = \"999999\"\n",
    "        for child in children :\n",
    "            if(child.dep_ == \"nsubjpass\"):\n",
    "                A = child.text\n",
    "\n",
    "            if(child.dep_ == \"advmod\"):\n",
    "                M = child.text\n",
    "\n",
    "        if(A != \"999999\" and M != \"999999\"):\n",
    "            \n",
    "            if M in pos:\n",
    "                sentiment = 1\n",
    "            elif M in neg:\n",
    "                sentiment = -1\n",
    "            else:\n",
    "                sentiment =0\n",
    "            rule4_pairs.append((A, M,sentiment))\n",
    "\n",
    "    ## FIFTH RULE OF DEPENDANCY PARSE -\n",
    "    ## M - Sentiment modifier || A - Aspect\n",
    "\n",
    "    #Complement of a copular verb - A is a child of M with relationship of nsubj, while\n",
    "    # M has a child with relationship of cop\n",
    "\n",
    "    #Assumption - A verb will have only one NSUBJ and DOBJ\n",
    "\n",
    "    rule5_pairs = []\n",
    "    for token in doc:\n",
    "        children = token.children\n",
    "        A = \"999999\"\n",
    "        buf_var = \"999999\"\n",
    "        for child in children :\n",
    "            if(child.dep_ == \"nsubj\"):\n",
    "                A = child.text\n",
    "\n",
    "            if(child.dep_ == \"cop\"):\n",
    "                buf_var = child.text\n",
    "\n",
    "        if(A != \"999999\" and buf_var != \"999999\"):\n",
    "            \n",
    "            if M in pos:\n",
    "                sentiment = 1\n",
    "            elif M in neg:\n",
    "                sentiment = -1\n",
    "            else:\n",
    "                sentiment =0\n",
    "            rule5_pairs.append((A, token.text,sentiment))\n",
    "            \n",
    "    aspects = []\n",
    "    aspects = rule1_pairs + rule2_pairs + rule3_pairs +rule4_pairs +rule5_pairs\n",
    "    #'userid','itemid',[('aspect','sentimentword','1or0')]\n",
    "    result = (userid ,itemid, aspects)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_review_withtext_sample = df_review_withtext_sample.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_review_withtext_sample['sentiment']=''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2h 41min 22s, sys: 8.57 s, total: 2h 41min 31s\n",
      "Wall time: 2h 41min 39s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_review_withtext_sample['sentiment'] = df_review_withtext_sample.apply(lambda row: apply_extraction(row,nlp), axis=1)\n",
    "\n",
    "#df_review_withtext_sample.to_pickle('processed_with_sentiment.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_review_withtext_sample = pd.read_pickle('processed_with_sentiment.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11040177    (A3D0TUJC497YBC, B0176UF0W4, [(little, nice, 1)])\n",
       "10993076    (A1TK3ZYY13QSNI, B015F1HS8A, [(product, super,...\n",
       "7402889     (AP3NJCBFQ3CYT, B00W8BB9EA, [(woman, super, 1)...\n",
       "3778676     (AFJIKJ97WVNCU, B009GPPMWG, [(thigh, perfect, ...\n",
       "5388130     (A3F3U8TTMBAW4Z, B00I4VPF84, [(shoes, great, 1...\n",
       "Name: sentiment, dtype: object"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_review_withtext_sample['sentiment'].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['rating', 'vote', 'verified', 'reviewTime', 'reviewerID', 'itemid',\n",
       "       'style', 'reviewerName', 'reviewText', 'summary', 'unixReviewTime',\n",
       "       'image', 'preprocess_data', 'preprocess_str', 'sentiment'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_review_withtext_sample.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = df_review_withtext_sample[['reviewerID','itemid','rating','sentiment']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new.columns = ['user','item','rating','aspect-level sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/cornac/data/dataset.py:361: UserWarning: 1978 duplicated observations are removed!\n",
      "  warnings.warn(\"%d duplicated observations are removed!\" % dup_count)\n",
      "/usr/local/lib/python3.6/dist-packages/cornac/data/dataset.py:361: UserWarning: 74 duplicated observations are removed!\n",
      "  warnings.warn(\"%d duplicated observations are removed!\" % dup_count)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of aspects: 48581\n",
      "Total number of opinions: 43225\n"
     ]
    }
   ],
   "source": [
    "# Use Sentiment Modality for aspect-level sentiment data\n",
    "sentiment_list = df_new['aspect-level sentiment'].tolist()\n",
    "sentiment_modality = SentimentModality(data=sentiment_list)\n",
    "\n",
    "# construct rating as a list of tuples (user, item, rating)\n",
    "rating = list(df_new[['user','item','rating']].itertuples(index=False,name = None))\n",
    "\n",
    "\n",
    "rs = RatioSplit(\n",
    "    data=rating,\n",
    "    test_size=0.2,\n",
    "    exclude_unknowns=True,\n",
    "    sentiment=sentiment_modality,\n",
    "    verbose=VERBOSE,\n",
    "    seed=SEED,\n",
    ")\n",
    "print(\"Total number of aspects:\", rs.sentiment.num_aspects)\n",
    "print(\"Total number of opinions:\", rs.sentiment.num_opinions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11040177    (A3D0TUJC497YBC, B0176UF0W4, [(little, nice, 1)])\n",
       "10993076    (A1TK3ZYY13QSNI, B015F1HS8A, [(product, super,...\n",
       "7402889     (AP3NJCBFQ3CYT, B00W8BB9EA, [(woman, super, 1)...\n",
       "3778676     (AFJIKJ97WVNCU, B009GPPMWG, [(thigh, perfect, ...\n",
       "5388130     (A3F3U8TTMBAW4Z, B00I4VPF84, [(shoes, great, 1...\n",
       "Name: aspect-level sentiment, dtype: object"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new['aspect-level sentiment'].tail()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building matrices completed!\n",
      "iter: 1, loss: 38288860.000000\n",
      "iter: 2, loss: 3221199.250000\n",
      "iter: 3, loss: 2004746.375000\n",
      "iter: 4, loss: 1627539.750000\n",
      "iter: 5, loss: 1443958.875000\n",
      "iter: 6, loss: 1333440.375000\n",
      "iter: 7, loss: 1256828.875000\n",
      "iter: 8, loss: 1198523.000000\n",
      "iter: 9, loss: 1151384.750000\n",
      "iter: 10, loss: 1111627.875000\n",
      "iter: 11, loss: 1077114.000000\n",
      "iter: 12, loss: 1046738.187500\n",
      "iter: 13, loss: 1018803.875000\n",
      "iter: 14, loss: 993380.312500\n",
      "iter: 15, loss: 970017.937500\n",
      "iter: 16, loss: 948325.375000\n",
      "iter: 17, loss: 928149.312500\n",
      "iter: 18, loss: 909271.000000\n",
      "iter: 19, loss: 891541.562500\n",
      "iter: 20, loss: 874834.187500\n",
      "iter: 21, loss: 858993.062500\n",
      "iter: 22, loss: 844008.625000\n",
      "iter: 23, loss: 829755.625000\n",
      "iter: 24, loss: 816241.187500\n",
      "iter: 25, loss: 803311.750000\n",
      "iter: 26, loss: 791002.000000\n",
      "iter: 27, loss: 779228.687500\n",
      "iter: 28, loss: 767893.625000\n",
      "iter: 29, loss: 757039.375000\n",
      "iter: 30, loss: 746597.875000\n",
      "iter: 31, loss: 736571.562500\n",
      "iter: 32, loss: 726901.312500\n",
      "iter: 33, loss: 717582.625000\n",
      "iter: 34, loss: 708592.000000\n",
      "iter: 35, loss: 699933.250000\n",
      "iter: 36, loss: 691558.687500\n",
      "iter: 37, loss: 683482.875000\n",
      "iter: 38, loss: 675662.312500\n",
      "iter: 39, loss: 668057.812500\n",
      "iter: 40, loss: 660689.125000\n",
      "iter: 41, loss: 653583.687500\n",
      "iter: 42, loss: 646659.312500\n",
      "iter: 43, loss: 639946.562500\n",
      "iter: 44, loss: 633440.250000\n",
      "iter: 45, loss: 627099.500000\n",
      "iter: 46, loss: 620956.875000\n",
      "iter: 47, loss: 614945.562500\n",
      "iter: 48, loss: 609120.812500\n",
      "iter: 49, loss: 603418.500000\n",
      "iter: 50, loss: 597884.750000\n",
      "iter: 51, loss: 592500.500000\n",
      "iter: 52, loss: 587242.125000\n",
      "iter: 53, loss: 582105.125000\n",
      "iter: 54, loss: 577131.062500\n",
      "iter: 55, loss: 572255.625000\n",
      "iter: 56, loss: 567504.812500\n",
      "iter: 57, loss: 562872.875000\n",
      "iter: 58, loss: 558337.500000\n",
      "iter: 59, loss: 553901.375000\n",
      "iter: 60, loss: 549592.312500\n",
      "iter: 61, loss: 545391.750000\n",
      "iter: 62, loss: 541283.312500\n",
      "iter: 63, loss: 537261.750000\n",
      "iter: 64, loss: 533318.125000\n",
      "iter: 65, loss: 529487.312500\n",
      "iter: 66, loss: 525763.812500\n",
      "iter: 67, loss: 524273.281250\n",
      "iter: 68, loss: 520613.312500\n",
      "iter: 69, loss: 517037.812500\n",
      "iter: 70, loss: 513536.625000\n",
      "iter: 71, loss: 510095.625000\n",
      "iter: 72, loss: 506702.781250\n",
      "iter: 73, loss: 503400.968750\n",
      "iter: 74, loss: 500169.750000\n",
      "iter: 75, loss: 496984.406250\n",
      "iter: 76, loss: 493862.093750\n",
      "iter: 77, loss: 490795.125000\n",
      "iter: 78, loss: 487789.968750\n",
      "iter: 79, loss: 484843.718750\n",
      "iter: 80, loss: 481964.125000\n",
      "iter: 81, loss: 479114.031250\n",
      "iter: 82, loss: 476329.656250\n",
      "iter: 83, loss: 473587.375000\n",
      "iter: 84, loss: 470890.187500\n",
      "iter: 85, loss: 468244.218750\n",
      "iter: 86, loss: 465637.906250\n",
      "iter: 87, loss: 463084.437500\n",
      "iter: 88, loss: 460570.812500\n",
      "iter: 89, loss: 458105.031250\n",
      "iter: 90, loss: 455678.031250\n",
      "iter: 91, loss: 453288.875000\n",
      "iter: 92, loss: 450937.093750\n",
      "iter: 93, loss: 448629.656250\n",
      "iter: 94, loss: 446363.281250\n",
      "iter: 95, loss: 444120.500000\n",
      "iter: 96, loss: 441934.343750\n",
      "iter: 97, loss: 439756.093750\n",
      "iter: 98, loss: 437631.687500\n",
      "iter: 99, loss: 435537.250000\n",
      "iter: 100, loss: 433464.031250\n",
      "Optimization finished!\n",
      "\n",
      "TEST:\n",
      "...\n",
      "    |    AUC |  F1@50 |    MRR | NDCG@50 | Precision@50 | Recall@50 | Train (s) |   Test (s)\n",
      "--- + ------ + ------ + ------ + ------- + ------------ + --------- + --------- + ----------\n",
      "EFM | 0.5867 | 0.0000 | 0.0000 |  0.0000 |       0.0000 |    0.0001 |  149.0052 | 27841.9433\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from cornac.models import EFM, MTER, NMF, BPR\n",
    "VERBOSE = True\n",
    "efm = EFM(\n",
    "  num_explicit_factors=20,\n",
    "  num_latent_factors=30,\n",
    "  num_most_cared_aspects=10,\n",
    "  rating_scale=5.0,\n",
    "  alpha=0.85,\n",
    "  lambda_x=1,\n",
    "  lambda_y=1,\n",
    "  lambda_u=0.01,\n",
    "  lambda_h=0.01,\n",
    "  lambda_v=0.01,\n",
    "  max_iter=100,\n",
    "  verbose=VERBOSE,\n",
    "  seed=SEED,\n",
    "  name=f\"EFM\",\n",
    ")\n",
    "\n",
    "eval_metrics = [\n",
    "    cornac.metrics.AUC(),\n",
    "    cornac.metrics.FMeasure(k=50),\n",
    "    cornac.metrics.Recall(k=50),\n",
    "    cornac.metrics.NDCG(k=50),\n",
    "    cornac.metrics.MRR(),\n",
    "    cornac.metrics.Precision(k=50),\n",
    "]\n",
    "\n",
    "cornac.Experiment(\n",
    "  eval_method=rs, models=[efm], metrics=eval_metrics\n",
    ").run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_aspect_map = {v:k for k, v in rs.sentiment.aspect_id_map.items()}\n",
    "id_opinion_map = {v:k for k, v in rs.sentiment.opinion_id_map.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "efm = EFM()\n",
    "efm.train_set = rs.train_set\n",
    "_, X, Y = efm._build_matrices(rs.train_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>buttons</th>\n",
       "      <th>jacket</th>\n",
       "      <th>think</th>\n",
       "      <th>side</th>\n",
       "      <th>purse</th>\n",
       "      <th>bag</th>\n",
       "      <th>reviews</th>\n",
       "      <th>bags</th>\n",
       "      <th>quality</th>\n",
       "      <th>luggage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>User 1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>User 2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>User 3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.620593</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>User 4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>User 5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>User 6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.848469</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>User 7</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>User 8</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>User 9</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>User 10</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>User 11</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.848469</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>User 12</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         buttons  jacket  think  side  purse  bag  reviews  bags   quality  \\\n",
       "User 1       0.0     0.0    0.0   0.0    0.0  0.0      0.0   0.0  0.000000   \n",
       "User 2       0.0     0.0    0.0   0.0    0.0  0.0      0.0   0.0  0.000000   \n",
       "User 3       0.0     0.0    0.0   0.0    0.0  0.0      0.0   0.0  4.620593   \n",
       "User 4       0.0     0.0    0.0   0.0    0.0  0.0      0.0   0.0  0.000000   \n",
       "User 5       0.0     0.0    0.0   0.0    0.0  0.0      0.0   0.0  0.000000   \n",
       "User 6       0.0     0.0    0.0   0.0    0.0  0.0      0.0   0.0  2.848469   \n",
       "User 7       0.0     0.0    0.0   0.0    0.0  0.0      0.0   0.0  0.000000   \n",
       "User 8       0.0     0.0    0.0   0.0    0.0  0.0      0.0   0.0  0.000000   \n",
       "User 9       0.0     0.0    0.0   0.0    0.0  0.0      0.0   0.0  0.000000   \n",
       "User 10      0.0     0.0    0.0   0.0    0.0  0.0      0.0   0.0  0.000000   \n",
       "User 11      0.0     0.0    0.0   0.0    0.0  0.0      0.0   0.0  2.848469   \n",
       "User 12      0.0     0.0    0.0   0.0    0.0  0.0      0.0   0.0  0.000000   \n",
       "\n",
       "         luggage  \n",
       "User 1       0.0  \n",
       "User 2       0.0  \n",
       "User 3       0.0  \n",
       "User 4       0.0  \n",
       "User 5       0.0  \n",
       "User 6       0.0  \n",
       "User 7       0.0  \n",
       "User 8       0.0  \n",
       "User 9       0.0  \n",
       "User 10      0.0  \n",
       "User 11      0.0  \n",
       "User 12      0.0  "
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_users = 12\n",
    "n_aspects =10\n",
    "pd.DataFrame(\n",
    "  data=X[:n_users, :n_aspects].A,\n",
    "  index=[f\"User {u + 1}\" for u in np.arange(n_users)],\n",
    "  columns=[f\"{id_aspect_map[i]}\" for i in np.arange(n_aspects)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>buttons</th>\n",
       "      <th>jacket</th>\n",
       "      <th>think</th>\n",
       "      <th>side</th>\n",
       "      <th>purse</th>\n",
       "      <th>bag</th>\n",
       "      <th>reviews</th>\n",
       "      <th>bags</th>\n",
       "      <th>quality</th>\n",
       "      <th>luggage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Item 1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Item 2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Item 3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.928055</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Item 4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.924234</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.523188</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Item 5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.075766</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.810297</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        buttons  jacket     think      side  purse  bag  reviews  bags  \\\n",
       "Item 1      0.0     0.0  0.000000  0.000000    0.0  0.0      0.0   0.0   \n",
       "Item 2      0.0     0.0  0.000000  0.000000    0.0  0.0      0.0   0.0   \n",
       "Item 3      0.0     0.0  0.000000  0.000000    0.0  0.0      3.0   0.0   \n",
       "Item 4      0.0     0.0  3.924234  3.000000    0.0  0.0      3.0   0.0   \n",
       "Item 5      0.0     0.0  0.000000  2.075766    0.0  0.0      3.0   0.0   \n",
       "\n",
       "         quality  luggage  \n",
       "Item 1  0.000000      0.0  \n",
       "Item 2  0.000000      0.0  \n",
       "Item 3  4.928055      0.0  \n",
       "Item 4  4.523188      0.0  \n",
       "Item 5  4.810297      0.0  "
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "n_items = 5\n",
    "n_aspects = 10\n",
    "pd.DataFrame(\n",
    "  data=Y[:n_items, :n_aspects].A,\n",
    "  index=[f\"Item {u + 1}\" for u in np.arange(n_items)],\n",
    "  columns=[f\"{id_aspect_map[i]}\" for i in np.arange(n_aspects)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('efm.pickle', 'wb') as handle:\n",
    "    pickle.dump(efm, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('efm.pickle', 'rb') as handle:\n",
    "    b = pickle.load(handle)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
